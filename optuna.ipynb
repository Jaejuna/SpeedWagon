{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HNnKr-49dQ9N"
      },
      "outputs": [],
      "source": [
        "!pip install datasets\n",
        "!pip install transformers==4.25.1\n",
        "!pip install Rouge"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#라이브러리 임포트\n",
        "\n",
        "import datasets\n",
        "import transformers\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import itertools\n",
        "import re\n",
        "import torch\n",
        "from rouge import Rouge\n",
        "from transformers import (\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    AutoTokenizer,\n",
        "    Seq2SeqTrainingArguments,\n",
        "    Seq2SeqTrainer,\n",
        "    DataCollatorForSeq2Seq,\n",
        "    TrainingArguments,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    EarlyStoppingCallback\n",
        "\n",
        ")\n",
        "from datasets import Dataset\n",
        "from tqdm import tqdm\n",
        "import glob\n",
        "import json\n",
        "import os"
      ],
      "metadata": {
        "id": "F_J1p3iBdlo7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#데이터 로드\n",
        "train = pd.read_csv('/content/drive/MyDrive/카카오톡일상대화/train_new_2.csv')\n",
        "valid = pd.read_csv('/content/drive/MyDrive/카카오톡일상대화/valid_new_2.csv')"
      ],
      "metadata": {
        "id": "sw7boILIdpmf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_sentence(sentence):\n",
        "    sentence = sentence.lower() # 텍스트 소문자화\n",
        "    sentence = re.sub(r'[ㄱ-ㅎㅏ-ㅣ]+[/ㄱ-ㅎㅏ-ㅣ]', '', sentence) # 여러개 자음과 모음을 삭제한다.\n",
        "    sentence = re.sub(\"[^가-힣a-z0-9#@,-]\", \" \", sentence) # 영어 외 문자(숫자, 특수문자 등) 공백으로 변환\n",
        "    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 여러개 공백을 하나의 공백으로 바꿉니다.\n",
        "    sentence = sentence.strip() # 문장 양쪽 공백 제거\n",
        "    \n",
        "    return sentence\n",
        "\n",
        "def data_process(data):\n",
        "  # 전체 Text 데이터에 대한 전처리 (1)\n",
        "  text = []\n",
        "\n",
        "  for data_text in tqdm(data):\n",
        "    text.append(preprocess_sentence(data_text))\n",
        "  \n",
        "  return text"
      ],
      "metadata": {
        "id": "G43Ml_05dslt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 가공\n",
        "train_texts = data_process(train['Text'])\n",
        "val_texts = data_process(valid['Text'])\n",
        "\n",
        "train_df = pd.DataFrame(zip(train_texts,train['Summary']), columns=['Text', 'Summary'])\n",
        "val_df = pd.DataFrame(zip(val_texts,valid['Summary']), columns=['Text', 'Summary'])"
      ],
      "metadata": {
        "id": "4KgGhPyudvzP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DF > data Set으로 전환\n",
        "train_data = Dataset.from_pandas(train_df) \n",
        "val_data = Dataset.from_pandas(val_df)\n",
        "test_samples = Dataset.from_pandas(val_df)\n",
        "\n",
        "print(train_data)\n",
        "print(val_data)\n",
        "print(test_samples)"
      ],
      "metadata": {
        "id": "UYQtGUsidzIN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model_checkpoints = \"/content/drive/MyDrive/인공지능/생성요약프로젝트/Model/KoBART/checkpoint/domain_adaptation/checkpoint-12500\"\n",
        "\n",
        "\n",
        "# 사용하고자 하는 허깅페이스의 모델\n",
        "model_checkpoints = 'gogamza/kobart-base-v2'\n",
        "#model_checkpoints = 'psyche/KoT5-summarization'\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoints)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoints)"
      ],
      "metadata": {
        "id": "DSDYHBvwd1SM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# max_input = 300\n",
        "# max_target = 100\n",
        "\n",
        "# 기존\n",
        "max_input = 256\n",
        "max_target = 64\n",
        "\n",
        "# 기존\n",
        "# max_input = 512\n",
        "# max_target = 128\n",
        "\n",
        "\n",
        "\n",
        "ignore_index = -100# tokenizer.pad_token_id\n",
        "\n",
        "def add_ignored_data(inputs, max_len, ignore_index):\n",
        "  if len(inputs) < max_len:\n",
        "      pad = [ignore_index] *(max_len - len(inputs))  # ignore_index즉 -100으로 패딩을 만들 것인데 max_len - lne(inpu)\n",
        "      inputs = np.concatenate([inputs, pad])         # max_input만큼 -100으로 남은 공간을 패딩처리\n",
        "  else:\n",
        "      inputs = inputs[:max_len]                      # 최대 길이보다 인풋이 크다면 최대길이만큼만 가져오기\n",
        "\n",
        "  return inputs\n",
        "\n",
        "def add_padding_data(inputs, max_len):\n",
        "    pad_index = tokenizer.pad_token_id              # 입력의 크기를 동일하게 하기 위한 pad 토큰들의 인덱스 지정\n",
        "    if len(inputs) < max_len:\n",
        "        pad = [pad_index] *(max_len - len(inputs))\n",
        "        inputs = np.concatenate([inputs, pad])\n",
        "    else:\n",
        "        inputs = inputs[:max_len]\n",
        "\n",
        "    return inputs \n",
        "\n",
        "def preprocess_data(data_to_process):\n",
        "    label_id= []\n",
        "    label_ids = []\n",
        "    dec_input_ids = []\n",
        "    input_ids = []\n",
        "    bos = tokenizer('')['input_ids']\n",
        "    for i in range(len(data_to_process['Text'])):\n",
        "        input_ids.append(add_padding_data(tokenizer.encode(data_to_process['Text'][i], add_special_tokens=False), max_input))\n",
        "    for i in range(len(data_to_process['Summary'])):\n",
        "        label_id.append(tokenizer.encode(data_to_process['Summary'][i]))  \n",
        "        label_id[i].append(tokenizer.eos_token_id)   \n",
        "        dec_input_id = bos\n",
        "        dec_input_id += label_id[i][:-1]\n",
        "        dec_input_ids.append(add_padding_data(dec_input_id, max_target))  \n",
        "    for i in range(len(data_to_process['Summary'])):\n",
        "        label_ids.append(add_ignored_data(label_id[i], max_target, ignore_index))\n",
        "   \n",
        "    return {'input_ids': input_ids,\n",
        "            'attention_mask' : (np.array(input_ids) != tokenizer.pad_token_id).astype(int),\n",
        "            'decoder_input_ids': dec_input_ids,\n",
        "            'decoder_attention_mask': (np.array(dec_input_ids) != tokenizer.pad_token_id).astype(int),\n",
        "            'labels': label_ids}\n",
        "\n",
        "train_tokenize_data = train_data.map(preprocess_data, batched = True, remove_columns=['Text', 'Summary'])\n",
        "val_tokenize_data = val_data.map(preprocess_data, batched = True, remove_columns=['Text', 'Summary'])"
      ],
      "metadata": {
        "id": "Km5E26AUd4Sc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model) # 데이터 일괄 처리?"
      ],
      "metadata": {
        "id": "MWapLtFzd69t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna"
      ],
      "metadata": {
        "id": "iyDbb3W-d8Fo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#여기서 output_dir이거 따로 설정\n",
        "#logging_dir 이것도 따로 바꿔야 함\n",
        "\n",
        "\n",
        "import optuna\n",
        "\n",
        "def objective(trial: optuna.Trial):\n",
        "    model = model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoints)\n",
        "    \n",
        "    training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir='/content/drive/MyDrive/카카오톡일상대화/results',\n",
        "    \n",
        "    \n",
        "    num_train_epochs=3,  # demo\n",
        "       \n",
        "    do_train=True,\n",
        "    do_eval=True,\n",
        "    \n",
        "    #배치사이즈를 128,256으로 할떄는 여전히 안됨\n",
        "    #16, 32\n",
        "    \n",
        "    per_device_train_batch_size=16,  # demo\n",
        "    per_device_eval_batch_size=32,\n",
        "    \n",
        "    # learning_rate=trial.suggest_loguniform(\"learning_rate\", low=4e-5, high=0.01),\n",
        "    learning_rate=trial.suggest_categorical('learning_rate', [3e-04,3e-05,3e-06]),\n",
        "    # weight_decay=trial.suggest_loguniform(\"weight_decay\", 4e-5, 0.01),\n",
        "    weight_decay=trial.suggest_categorical(\"weight_decay\", [0.1,0.2,0.01]),\n",
        "    \n",
        "    predict_with_generate=True, # 생성기능을 사용하고 싶다고 지정한다.\n",
        "    ##logging_dir=\"/content/drive/MyDrive/인공지능/생성요약프로젝트/Model/KoBART/logs2\",\n",
        "    logging_dir='/content/drive/MyDrive/카카오톡일상대화/logs',\n",
        "    \n",
        "    save_total_limit=3,\n",
        "    load_best_model_at_end = True,\n",
        "    logging_strategy = 'epoch',\n",
        "    evaluation_strategy  = 'epoch',\n",
        "    save_strategy ='epoch',\n",
        "    )\n",
        "        \n",
        "    trainer = Seq2SeqTrainer(\n",
        "    model, \n",
        "    training_args,\n",
        "    train_dataset=train_tokenize_data,\n",
        "    eval_dataset=val_tokenize_data,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    )\n",
        "    result = trainer.train()\n",
        "    return result.training_loss"
      ],
      "metadata": {
        "id": "kEkaX5S3d-DO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#여기가 하이퍼 튜닝 하는곳 시간이 오래 걸린다\n",
        "\n",
        "study = optuna.create_study(study_name=\"hyper-parameter-search\", direction=\"minimize\")\n",
        "\n",
        "#기존 시도는 15\n",
        "study.optimize(func=objective, n_trials=3)\n",
        "print(study.best_value)\n",
        "print(study.best_params)\n",
        "print(study.best_trial)"
      ],
      "metadata": {
        "id": "q7OUNejNeA-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optuna.visualization.plot_param_importances(study)"
      ],
      "metadata": {
        "id": "TaAe8d7CeC0H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optuna.visualization.plot_optimization_history(study)"
      ],
      "metadata": {
        "id": "5HJkbbAPeEaW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}